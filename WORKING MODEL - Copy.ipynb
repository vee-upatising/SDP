{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     A B A B Z\n",
       "1     A C A C Z\n",
       "2     A D A D Z\n",
       "3     C B C B Z\n",
       "4     A A A B Z\n",
       "5     C C C B Z\n",
       "6     A B C A Z\n",
       "7     D B D B Z\n",
       "8     D B A D Z\n",
       "9     C A D B Z\n",
       "10    D G A D Z\n",
       "11    E E E F Z\n",
       "12    F F A G Z\n",
       "13    E B A G Z\n",
       "14    B B B B Z\n",
       "15    A G B E Z\n",
       "16    E G B B Z\n",
       "17    F F F F Z\n",
       "18    A F E F Z\n",
       "19    B F E G Z\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv = pd.read_csv('data.csv', header = None)\n",
    "data = csv.iloc[:, 0]\n",
    "target = csv.iloc[:, 1]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_lexicon = \" \".join(target).split()\n",
    "def create_lookup_tables(words):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param words: Input list of words\n",
    "    :return: A tuple of dicts.  The first dict maps a vocab word to and integeter\n",
    "             The second maps an integer back to to the vocab word\n",
    "    \"\"\"\n",
    "    word_counts = Counter(words)\n",
    "    sorted_vocab = sorted(word_counts, key=word_counts.get, reverse=True)\n",
    "    int_to_vocab = {ii: word for ii, word in enumerate(sorted_vocab, 1)}\n",
    "    vocab_to_int = {word: ii for ii, word in int_to_vocab.items()}\n",
    "\n",
    "    return vocab_to_int, int_to_vocab\n",
    "vocab_to_int, int_to_vocab = create_lookup_tables(full_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = []\n",
    "for x in range(len(data)):\n",
    "    d.append(data[x].split(\" \"))\n",
    "for x in range(len(d)):\n",
    "    for z in range(len(d[x])):\n",
    "        if d[x][z] == 'A':\n",
    "            d[x][z] = vocab_to_int['A']\n",
    "        elif d[x][z] == 'B':\n",
    "            d[x][z] = vocab_to_int['B']\n",
    "        elif d[x][z] == 'C':\n",
    "            d[x][z] = vocab_to_int['C']\n",
    "        elif d[x][z] == 'D':\n",
    "            d[x][z] = vocab_to_int['D']\n",
    "        elif d[x][z] == 'E':\n",
    "            d[x][z] = vocab_to_int['E']\n",
    "        elif d[x][z] == 'F':\n",
    "            d[x][z] = vocab_to_int['F']\n",
    "        elif d[x][z] == 'G':\n",
    "            d[x][z] = vocab_to_int['G']\n",
    "        elif d[x][z] == 'Z':\n",
    "            d[x][z] = vocab_to_int['Z']\n",
    "d = [d]\n",
    "d = np.array(d)\n",
    "d = d.reshape(20, 1, 5) / 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "for x in range(len(target)):\n",
    "    a.append(target[x].split(\" \"))\n",
    "for x in range(len(a)):\n",
    "    for z in range(len(a[x])):\n",
    "        if a[x][z] == 'A':\n",
    "            a[x][z] = int(vocab_to_int['A'])\n",
    "        elif a[x][z] == 'B':\n",
    "            a[x][z] = vocab_to_int['B']\n",
    "        elif a[x][z] == 'C':\n",
    "            a[x][z] = vocab_to_int['C']\n",
    "        elif a[x][z] == 'D':\n",
    "            a[x][z] = vocab_to_int['D']\n",
    "        elif a[x][z] == 'E':\n",
    "            a[x][z] = vocab_to_int['E']\n",
    "        elif a[x][z] == 'F':\n",
    "            a[x][z] = vocab_to_int['F']\n",
    "        elif a[x][z] == 'G':\n",
    "            a[x][z] = vocab_to_int['G']\n",
    "        elif a[x][z] == 'Z':\n",
    "            a[x][z] = vocab_to_int['Z']\n",
    "        elif a[x][z] == '0':\n",
    "            a[x][z] = 0\n",
    "\n",
    "a = [a]\n",
    "a = np.array(a)\n",
    "a = a.reshape(20, 1, 10) / 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2., 1., 2., 1., 4.]],\n",
       "\n",
       "       [[2., 6., 2., 6., 4.]],\n",
       "\n",
       "       [[2., 7., 2., 7., 4.]],\n",
       "\n",
       "       [[6., 1., 6., 1., 4.]],\n",
       "\n",
       "       [[2., 2., 2., 1., 4.]],\n",
       "\n",
       "       [[6., 6., 6., 1., 4.]],\n",
       "\n",
       "       [[2., 1., 6., 2., 4.]],\n",
       "\n",
       "       [[7., 1., 7., 1., 4.]],\n",
       "\n",
       "       [[7., 1., 2., 7., 4.]],\n",
       "\n",
       "       [[6., 2., 7., 1., 4.]],\n",
       "\n",
       "       [[7., 9., 2., 7., 4.]],\n",
       "\n",
       "       [[8., 8., 8., 5., 4.]],\n",
       "\n",
       "       [[5., 5., 2., 9., 4.]],\n",
       "\n",
       "       [[8., 1., 2., 9., 4.]],\n",
       "\n",
       "       [[1., 1., 1., 1., 4.]],\n",
       "\n",
       "       [[2., 9., 1., 8., 4.]],\n",
       "\n",
       "       [[8., 9., 1., 1., 4.]],\n",
       "\n",
       "       [[5., 5., 5., 5., 4.]],\n",
       "\n",
       "       [[2., 5., 8., 5., 4.]],\n",
       "\n",
       "       [[1., 5., 8., 9., 4.]]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2., 1., 2., 1., 1., 1., 2., 4., 0., 0.]],\n",
       "\n",
       "       [[2., 6., 2., 6., 6., 6., 2., 4., 0., 0.]],\n",
       "\n",
       "       [[2., 7., 2., 7., 7., 7., 2., 4., 0., 0.]],\n",
       "\n",
       "       [[6., 1., 6., 1., 6., 1., 6., 1., 4., 0.]],\n",
       "\n",
       "       [[2., 2., 2., 1., 2., 2., 2., 1., 4., 0.]],\n",
       "\n",
       "       [[6., 6., 6., 1., 6., 6., 6., 1., 4., 0.]],\n",
       "\n",
       "       [[2., 1., 6., 2., 1., 6., 2., 4., 0., 0.]],\n",
       "\n",
       "       [[7., 1., 7., 1., 1., 1., 7., 4., 0., 0.]],\n",
       "\n",
       "       [[7., 1., 2., 7., 1., 7., 2., 4., 0., 0.]],\n",
       "\n",
       "       [[6., 2., 7., 1., 7., 2., 1., 7., 4., 0.]],\n",
       "\n",
       "       [[7., 9., 2., 7., 9., 7., 2., 7., 4., 0.]],\n",
       "\n",
       "       [[8., 8., 8., 5., 8., 8., 5., 4., 0., 0.]],\n",
       "\n",
       "       [[5., 2., 9., 5., 2., 9., 4., 0., 0., 0.]],\n",
       "\n",
       "       [[8., 1., 2., 9., 1., 2., 9., 4., 0., 0.]],\n",
       "\n",
       "       [[1., 1., 1., 1., 1., 1., 1., 1., 4., 0.]],\n",
       "\n",
       "       [[2., 9., 1., 8., 2., 2., 9., 1., 8., 4.]],\n",
       "\n",
       "       [[8., 1., 9., 8., 1., 9., 8., 1., 1., 4.]],\n",
       "\n",
       "       [[5., 5., 5., 5., 5., 5., 5., 5., 4., 0.]],\n",
       "\n",
       "       [[2., 5., 8., 5., 2., 5., 8., 5., 4., 0.]],\n",
       "\n",
       "       [[1., 5., 8., 9., 1., 5., 8., 9., 4., 0.]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting data into train and test sets. 3/4 train, 1/4 test.\n",
    "x_train,x_test,y_train,y_test = train_test_split(d, a, test_size=0.2, shuffle=False, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0916 03:04:29.716599  7780 deprecation_wrapper.py:119] From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0916 03:04:29.726572  7780 deprecation_wrapper.py:119] From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0916 03:04:29.728567  7780 deprecation_wrapper.py:119] From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0916 03:04:30.630054  7780 deprecation_wrapper.py:119] From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n",
      "W0916 03:04:30.635040  7780 deprecation.py:506] From D:\\Anaconda\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0916 03:04:30.752074  7780 deprecation_wrapper.py:119] From D:\\Anaconda\\lib\\site-packages\\keras\\optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 1, 128)            68608     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 1, 128)            131584    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 1, 128)            131584    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 1, 128)            131584    \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 1, 128)            131584    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 1, 128)            131584    \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (None, 1, 128)            131584    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1, 10)             1290      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1, 10)             110       \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1, 10)             110       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 1, 10)             0         \n",
      "=================================================================\n",
      "Total params: 859,622\n",
      "Trainable params: 859,622\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#getting the right tensor shape for this was a BITCH\n",
    "model = Sequential()  \n",
    "model.add(LSTM(128, input_shape=(1, 5),return_sequences=True, activation = 'relu'))\n",
    "model.add(LSTM(128, return_sequences=True, activation = 'tanh'))\n",
    "model.add(LSTM(128, return_sequences=True, activation = 'tanh'))\n",
    "model.add(LSTM(128, return_sequences=True, activation = 'tanh'))\n",
    "model.add(LSTM(128, return_sequences=True, activation = 'tanh'))\n",
    "model.add(LSTM(128, return_sequences=True, activation = 'tanh'))\n",
    "model.add(LSTM(128, return_sequences=True, activation = 'tanh'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(units=10))\n",
    "model.add(Dense(units=10))\n",
    "model.add(Dense(units=10))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss='mean_absolute_error', optimizer='adam')\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20 samples, validate on 4 samples\n",
      "Epoch 1/25\n",
      " - 0s - loss: 0.3585 - val_loss: 0.4925\n",
      "Epoch 2/25\n",
      " - 0s - loss: 0.3580 - val_loss: 0.4925\n",
      "Epoch 3/25\n",
      " - 0s - loss: 0.3578 - val_loss: 0.4925\n",
      "Epoch 4/25\n",
      " - 0s - loss: 0.3580 - val_loss: 0.4925\n",
      "Epoch 5/25\n",
      " - 0s - loss: 0.3576 - val_loss: 0.4922\n",
      "Epoch 6/25\n",
      " - 0s - loss: 0.3574 - val_loss: 0.4925\n",
      "Epoch 7/25\n",
      " - 0s - loss: 0.3576 - val_loss: 0.4922\n",
      "Epoch 8/25\n",
      " - 0s - loss: 0.3577 - val_loss: 0.4929\n",
      "Epoch 9/25\n",
      " - 0s - loss: 0.3574 - val_loss: 0.4928\n",
      "Epoch 10/25\n",
      " - 0s - loss: 0.3574 - val_loss: 0.4925\n",
      "Epoch 11/25\n",
      " - 0s - loss: 0.3569 - val_loss: 0.4925\n",
      "Epoch 12/25\n",
      " - 0s - loss: 0.3573 - val_loss: 0.4925\n",
      "Epoch 13/25\n",
      " - 0s - loss: 0.3572 - val_loss: 0.4921\n",
      "Epoch 14/25\n",
      " - 0s - loss: 0.3570 - val_loss: 0.4920\n",
      "Epoch 15/25\n",
      " - 0s - loss: 0.3568 - val_loss: 0.4921\n",
      "Epoch 16/25\n",
      " - 0s - loss: 0.3571 - val_loss: 0.4917\n",
      "Epoch 17/25\n",
      " - 0s - loss: 0.3568 - val_loss: 0.4914\n",
      "Epoch 18/25\n",
      " - 0s - loss: 0.3567 - val_loss: 0.4915\n",
      "Epoch 19/25\n",
      " - 0s - loss: 0.3569 - val_loss: 0.4911\n",
      "Epoch 20/25\n",
      " - 0s - loss: 0.3564 - val_loss: 0.4906\n",
      "Epoch 21/25\n",
      " - 0s - loss: 0.3564 - val_loss: 0.4906\n",
      "Epoch 22/25\n",
      " - 0s - loss: 0.3563 - val_loss: 0.4904\n",
      "Epoch 23/25\n",
      " - 0s - loss: 0.3560 - val_loss: 0.4903\n",
      "Epoch 24/25\n",
      " - 0s - loss: 0.3557 - val_loss: 0.4901\n",
      "Epoch 25/25\n",
      " - 0s - loss: 0.3557 - val_loss: 0.4900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1d58703feb8>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(d, a, epochs=25, batch_size=5, verbose=2,validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "guess = np.array([2, 1, 1, 1, 4])\n",
    "guess = guess.reshape(1,1,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.predict_on_batch(guess)*8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2. 1. 0. 1. 1. 1. 1. 1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "array = np.around(predict[0][0])\n",
    "print(array)\n",
    "new = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for z in range(len(array)):\n",
    "    if array[z] == 1:\n",
    "        new.append(int_to_vocab[1])\n",
    "    elif array[z] == 2:\n",
    "        new.append(int_to_vocab[2])\n",
    "    elif array[z] == 3:\n",
    "        new.append(int_to_vocab[3])\n",
    "    elif array[z] == 4:\n",
    "        new.append(int_to_vocab[4])\n",
    "    elif array[z] == 5:\n",
    "        new.append(int_to_vocab[5])\n",
    "    elif array[z] == 6:\n",
    "        new.append(int_to_vocab[6])\n",
    "    elif array[z] == 7:\n",
    "        new.append(int_to_vocab[7])\n",
    "    elif array[z] == 8:\n",
    "        new.append(int_to_vocab[8])\n",
    "    elif array[z] == 0:\n",
    "        new.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'B', 0, 'B', 'B', 'B', 'B', 'B', 0, 0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "# Creates a HDF5 file 'my_model.h5'\n",
    "model.save('my_model.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
